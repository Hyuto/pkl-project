---
title: "Main Notebook"
author: "Wahyu Setianto"
date: "11/18/2021"
output: html_document
---

## Load Library

Load library yang akan digunakan untuk mengolah data.

```{r message=FALSE, warning=FALSE}
library(tm)
library(stringr)
library(textclean)
library(katadasaR)
library(tokenizers)
library(tidyverse)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(wordcloud2)
```

## Reading Datasets

Membaca (loading) dataset yang akan digunakan ke dalam memmory. Dataset yang digunakan adalah dataset hasil `Sampling` notebook yang telah dilakukan `labelling`.

```{r}
data <- read_csv("../data/sample-data.csv")
glimpse(data)
```

### Ubah tipe data

Mengubah tipe data pada tiap kolom yang dianggap tidak sesuai.

```
service : chr -> fct
date    : chr -> date 
```

```{r}
data <- data %>%
  mutate(service = as.factor(service), 
         date = as.Date(date, "%m/%d/%Y"),
         label = as.factor(label))
glimpse(data)
```
## Data Exploration

Melakukan explorasi terhadap data dengan visualisasi

**Social Media Platform**

```{r}
ggplot(data, aes(x = service, fill=service)) +
  geom_bar(stat="count") +
  ggtitle("Social Media Platform") +
  theme_minimal()
```

**OCEAN Frequency**

```{r}
ggplot(data, aes(x = label, fill = label)) +
  geom_bar(stat="count") +
  coord_flip() +
  ggtitle("Frekuensi Setiap Kelas") +
  theme_minimal()
```

## Cleanning Text

Membersihkan data teks agar data menjadi siap olah.

### Mengganti Emoji & Menghapus HTML tag

Mengganti emoji dengan kalimat dan Menghapus HTML tag yang ada di dalam suatu teks. 

**Contoh**

```{r}
"Halo ❤️ <br> Lagi apanih?" %>%
  replace_emoji() %>%
  replace_html()
```

### 2. Menghapus URL

Menghapus url yang terdapat di dalam teks.

**contoh**

```{r}
"https://google.co.id adalah halaman awal google indonesia" %>%
  replace_url()
```

### 3. Mengganti kata slang

Mengganti kata slang yang terdapat dalam tweet. Kata slang sangan umum untuk di gunakan di media sosial, untuk menangani hal ini maka kata slang perlu dibenarkan kedalam kata formal. Digunakan `colloquial-indonesian-lexicon.csv` dari [nasalsabila/kamus-alay](https://github.com/nasalsabila/kamus-alay) yang berisi daftar kata slang yang umum di gunakan di media sosial dan bentuk formalnya.

```{r}
lexicon <- read_csv("https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv")
glimpse(lexicon)
```

**Contoh**

```{r}
"Budi adalah tmn dari dani yg suka makan di taman" %>%
  replace_internet_slang(., slang = paste0("\\b", lexicon$slang, "\\b"),
                         replacement = lexicon$formal, ignore.case = TRUE)
```

### 4. Stemming & Menghapus Stopwords

Melakukan stemming dengan library `katadasaR` dan menghapus `stopwords` yang ada di dalam teks.

**Indonesian Stopwords**

List kata - kata yang termasuk stopwords di dalam bahasa indonesia. Didapatkan dari [aliakbars/bilp](https://github.com/aliakbars/bilp)

```{r warning=FALSE}
stopwords.id <- readLines("https://raw.githubusercontent.com/aliakbars/bilp/master/stoplist")
head(stopwords.id)
```

**Stemming function**

```{r}
stemming <- function(x){
  return(paste(lapply(x,katadasar),collapse = " "))
}
```

**Contoh**

```{r}
"Menikmati acara tv dengan melihat jadwal yang tayang secara lengkap" %>%
  tokenize_words(., stopwords = stopwords.id) %>%
  lapply(., stemming) %>%
  as.character()
```

### 5. Normalisasi Teks

Menormalisasi teks dengan beberapa tahapan.

1. undercase
2. menghapus newline
3. menghapus punctuation
4. menghandle huruf yang berulang disuatu kata
5. menghapus whitespace

```{r}
"Halo    Lagi apanih?\n kebetulan gw lagi kosong bangettt nih..." %>%
  tolower() %>%
  gsub("\\\\n", " ", .) %>% 
  gsub("[[:punct:]]", " ", .) %>% 
  replace_word_elongation() %>%
  replace_white() %>%
  str_trim()
```

**Running all text cleaning**

Running semua metode text cleaning pada sample dataset

```{r}
data$message <- data$message %>%
  replace_emoji() %>% # replace emoji dengan kalimat
  replace_html() %>% # menghapus kode
  replace_url() %>% # menghapus url
  replace_internet_slang(.,
    slang = paste0("\\b", lexicon$slang, "\\b"),
    replacement = lexicon$formal, ignore.case = TRUE
  ) %>% # menghapus slang
  tolower() %>% # mengubah menjadi huruf non-kapital
  gsub("\\\\n", " ", .) %>% # mengganti newline dengan spasi
  gsub("[[:punct:]]", " ", .) %>% # mengganti punctuation dengan spasi
  replace_word_elongation() %>% # menghandle huruf yang berulang
  replace_white() %>% # menghapus whitespace
  str_trim() %>%
  tokenize_words(., stopwords = stopwords.id) %>% # menghapus stopwords
  lapply(., stemming) %>% # stemming
  as.character()

head(data$message)
```

**Wordclouds**

Membuat wordcloud untuk menlihat frekuensi sebaran kata dalam data.

```{r}
vcorpus <- Corpus(x = VectorSource(data$message))
temp.df <- tidy(DocumentTermMatrix(vcorpus))
temp.df <- temp.df[order(-temp.df$count), c(2,3)]
wordcloud2(temp.df, size=1, color = "random-dark", )
```

## Modelling

```{r}
corpus <- subset(data, select = c("message", "label"))  %>%
  mutate(Id = row_number())

set.seed(2021)
text_split <- initial_split(corpus, prop = 0.8)
training_set <- training(text_split)
test_set <- testing(text_split)
```

```{r}
text_recipe <- recipe(label ~ ., data = training_set) %>% 
  update_role(Id, new_role = "ID") %>% 
  step_tokenize(message) %>% 
  step_tfidf(message)
```

**logistic regression**

```{r}
log.reg <- logistic_reg(mode = "classification") %>%
  set_engine("LiblineaR")
log.reg.wf <- workflows::workflow() %>% 
  add_recipe(text_recipe) %>% 
  add_model(log.reg)

log.reg.model <- fit(log.reg.wf, training_set)
```

**svm**

```{r}
svm <- svm_rbf("classification") %>% 
  set_engine("kernlab")
svm.wf <- workflows::workflow() %>% 
  add_recipe(text_recipe) %>% 
  add_model(svm)

svm.model <- fit(svm.wf, training_set)
```

**naive bayes**

```{r}
nb <- discrim::naive_Bayes() %>% 
  set_engine("naivebayes") %>% 
  set_mode("classification")
nb.wf <- workflows::workflow() %>% 
  add_recipe(text_recipe) %>% 
  add_model(nb)

nb.model <- fit(nb.wf, training_set)
```

**Goodness of Fit**

```{r}
predictions_logreg <- predict(log.reg.model, test_set)
predictions_svm <- predict(svm.model, test_set)
predictions_nb <- predict(nb.model, test_set)

logreg.acc <- bind_cols(test_set, predictions_logreg) %>% 
  accuracy(truth = label, estimate = .pred_class)
svm.acc <- bind_cols(test_set, predictions_svm) %>% 
  accuracy(truth = label, estimate = .pred_class)
nb.acc <- bind_cols(test_set, predictions_nb) %>% 
  accuracy(truth = label, estimate = .pred_class)

res <- data.frame(list("LogReg"=logreg.acc$.estimate[1],
                       "SVM"=svm.acc$.estimate[1],
                       "NB"=nb.acc$.estimate[1]))
row.names(res) <- c("Accuracy")
res
```

**Confussion Matrix**

```{r}
cm <- bind_cols(test_set, predictions_logreg) %>% 
  conf_mat(truth = label, estimate = .pred_class)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8", high = "#2E86C1") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


